{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1. Loading and cleaning data, \n",
    "2. Feature engineering, \n",
    "3. HFT strategy, \n",
    "4. Predication of last 5 mins of each trading day"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and cleaning the data of 002521 and 300132 for 4 months"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Droping the entire columns that consist of zeros, NaNs, and IDs, as these are not involved in subsequent calculations.\n",
    "- Changing the storage format of all data to reduce memory consumption.\n",
    "- Converting the DataTime column to a time series and arranging all data sequentially based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_h5_files(folder_path, file_pattern='*.h5'):\n",
    "\n",
    "    # Generate the file-matching pattern\n",
    "    pattern = os.path.join(folder_path, file_pattern)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    # Iterate over each matching file\n",
    "    for file_path in glob.glob(pattern):\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            columns = list(f.keys())\n",
    "            data_dict = {}\n",
    "\n",
    "            # Build a dictionary { column_name: numpy_array_of_values }\n",
    "            for col in columns:\n",
    "                data_dict[col] = f[col][:]\n",
    "\n",
    "            # Convert this file's data into a DataFrame\n",
    "            df_temp = pd.DataFrame(data_dict)\n",
    "\n",
    "        df_list.append(df_temp)\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    if df_list:\n",
    "        df_combined = pd.concat(df_list, ignore_index=True)\n",
    "    else:\n",
    "        # In case no files match, return an empty DataFrame\n",
    "        df_combined = pd.DataFrame()\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "\n",
    "    columns_all_zero = [col for col in df.columns if (df[col] == 0).all()]\n",
    "\n",
    "    df.drop(columns=columns_all_zero, inplace=True)\n",
    "\n",
    "    if 'Nano' in df.columns:\n",
    "        df.drop(columns=['Nano'], inplace=True)\n",
    "    if 'TradingDay' in df.columns:\n",
    "        df.drop(columns=['TradingDay'], inplace=True)\n",
    "    if 'InstrumentID' in df.columns:\n",
    "        df.drop(columns=['InstrumentID'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def sort_by_datatime(df):\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['DataTime']):\n",
    "        try:\n",
    "\n",
    "            df['DataTime'] = pd.to_datetime(df['DataTime'], format='%Y%m%d%H%M%S%f')\n",
    "        except ValueError:\n",
    "\n",
    "            pass\n",
    "\n",
    "    df.sort_values(by='DataTime', inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def reduce_column_size(df):\n",
    "\n",
    "    df_reduced = df.copy()\n",
    "\n",
    "    int_cols = df_reduced.select_dtypes(include=['int64']).columns\n",
    "    for col in int_cols:\n",
    "        df_reduced[col] = pd.to_numeric(df_reduced[col], downcast='integer')\n",
    "\n",
    "    float_cols = df_reduced.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df_reduced[col] = pd.to_numeric(df_reduced[col], downcast='float')\n",
    "\n",
    "    return df_reduced\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_002521 = combine_h5_files('interview', '002521*.h5') # load\n",
    "df_300132 = combine_h5_files('interview', '300132*.h5')\n",
    "df_002521 = clean_dataframe(df_002521) # clean\n",
    "df_300132 = clean_dataframe(df_300132)\n",
    "df_002521 = sort_by_datatime(df_002521) # timestamp\n",
    "df_300132 = sort_by_datatime(df_300132)\n",
    "df_002521 = reduce_column_size(df_002521) # reduce RAM\n",
    "df_300132 = reduce_column_size(df_300132)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features were developed to reflect the imbalance, liquidity, price dynamics, and statistics of volume and prices\n",
    "- imbalance = bid_volume1-10/(bid_volume1-10 + ask_volume1-10)\n",
    "- delta_imbalance = imbalance - (last imbalance)\n",
    "- rolling_delta_imb = (sum of delta_imbalance in past 5 rows)/5\n",
    "- returns = price.pct_change()\n",
    "- volatilities in different period 10s, 1min, 1Hour, 1D, 7D, 30D\n",
    "- midprice = (BidPrice1 + AskPrice1)/2\n",
    "- spread = (AskPrice1 - BidPrice1)\n",
    "- momentum = turnover * return\n",
    "- size_imbalance\" = total_bid_volume / total_bid_volume\n",
    "- spread_intensity = spread.diff()\n",
    "- 'market_urgency' = 'spread * liquidity_imbalance\n",
    "- order_book_imbalance = (total_ask_volume - total_bid_volume)/(total_ask_volume + total_bid_volume + 1e-6)\n",
    "- weighted_bid_price based on the bid_volume1-10\n",
    "- weighted_ask_price based on the ask_volume1-10\n",
    "- bid_price_gap = weighted_bid_price - BidPrice1\n",
    "- ask_price_gap = weighted_ask_price = AskPrice1\n",
    "- mean, std, medium of bidprice\n",
    "- mean, std, medium of askprice\n",
    "- top3 ask volume sum\n",
    "- top3 bid volume sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_level2_imbalance(df, bid_levels=10, ask_levels=10, time_col='DataTime'):\n",
    "\n",
    "    df = df.sort_values(by=time_col).copy()\n",
    "\n",
    "    bid_volume_cols = [f\"BidVolume{i}\" for i in range(1, bid_levels + 1) if f\"BidVolume{i}\" in df.columns]\n",
    "    ask_volume_cols = [f\"AskVolume{i}\" for i in range(1, ask_levels + 1) if f\"AskVolume{i}\" in df.columns]\n",
    "\n",
    "    df[\"total_bid_volume\"] = df[bid_volume_cols].sum(axis=1)\n",
    "    df[\"total_ask_volume\"] = df[ask_volume_cols].sum(axis=1)\n",
    "    df[\"total_volume\"] = df[\"total_bid_volume\"] + df[\"total_ask_volume\"]\n",
    "    df[\"imbalance\"] = df[\"total_bid_volume\"] / (df[\"total_bid_volume\"] + df[\"total_ask_volume\"])\n",
    "\n",
    "    df[\"imbalance\"].fillna(0.5, inplace=True)\n",
    "\n",
    "    df[\"delta_imbalance\"] = df[\"imbalance\"].diff()\n",
    "\n",
    "    df['rolling_delta_imb'] = df['delta_imbalance'].rolling(window=5).mean()\n",
    "    return df\n",
    "def add_rolling_volatility(\n",
    "    df,\n",
    "    time_col='DataTime',\n",
    "    price_col='LastPrice',\n",
    "    window='1D',\n",
    "    new_col='volatility',\n",
    "    use_log_returns=False\n",
    "):\n",
    "\n",
    "    df_local = df.copy()\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_local[time_col]):\n",
    "        df_local[time_col] = pd.to_datetime(df_local[time_col], errors='coerce')\n",
    "\n",
    "    df_local.sort_values(by=time_col, inplace=True)\n",
    "\n",
    "    if use_log_returns:\n",
    "        df_local['returns'] = np.log(df_local[price_col].pct_change() + 1)\n",
    "    else:\n",
    "        df_local['returns'] = df_local[price_col].pct_change()\n",
    "\n",
    "    df_local.dropna(subset=['returns'], inplace=True)\n",
    "\n",
    "    df_local[new_col] = (\n",
    "        df_local\n",
    "        .rolling(window=window, on=time_col)['returns']\n",
    "        .std()\n",
    "    )\n",
    "\n",
    "    return df_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_002521_signals = generate_level2_imbalance(df_002521, bid_levels=10, ask_levels=10, time_col='DataTime') #imbalance features\n",
    "df_300132_signals = generate_level2_imbalance(df_300132, bid_levels=10, ask_levels=10, time_col='DataTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='1D', new_col='daily_vol') # volatility features\n",
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='7D', new_col='weekly_vol')\n",
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='30D', new_col='monthly_vol')\n",
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='1H', new_col='hourly_vol')\n",
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='1T', new_col='min_vol')\n",
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='1S', new_col='sec_vol')\n",
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='10S', new_col='tenth_sec_vol')\n",
    "df_300132_signals = add_rolling_volatility(df_300132_signals, window='1D', new_col='daily_vol')\n",
    "df_300132_signals = add_rolling_volatility(df_300132_signals, window='7D', new_col='weekly_vol')\n",
    "df_300132_signals = add_rolling_volatility(df_300132_signals, window='30D', new_col='monthly_vol')\n",
    "df_300132_signals = add_rolling_volatility(df_300132_signals, window='1H', new_col='hourly_vol')\n",
    "df_300132_signals = add_rolling_volatility(df_300132_signals, window='1T', new_col='min_vol')\n",
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='1S', new_col='sec_vol')\n",
    "df_002521_signals = add_rolling_volatility(df_002521_signals, window='10S', new_col='tenth_sec_vol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_002521_signals = df_002521_signals[df_002521_signals['LastPrice'] != 0] # drop price that equals to 0\n",
    "df_300132_signals = df_300132_signals[df_300132_signals['LastPrice'] != 0]\n",
    "df_002521_signals.set_index('DataTime', inplace=True)\n",
    "df_300132_signals.set_index('DataTime', inplace=True)\n",
    "df_002521_signals ['momentum'] = df_002521_signals['Turnover']*df_002521_signals['returns'] # features of momentum, midprice, spread\n",
    "df_300132_signals ['momentum'] = df_300132_signals['Turnover']*df_300132_signals['returns']\n",
    "df_002521_signals ['midprice'] = (df_002521_signals['AskPrice1'] + df_002521_signals['BidPrice1'])/2\n",
    "df_002521_signals ['spread'] = df_002521_signals['AskPrice1'] - df_002521_signals['BidPrice1']\n",
    "df_300132_signals ['midprice'] = (df_300132_signals['AskPrice1'] + df_300132_signals['BidPrice1'])/2\n",
    "df_300132_signals ['spread'] = df_300132_signals['AskPrice1'] - df_300132_signals['BidPrice1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_002521_signals, df_300132_signals]: # spread + imbalance features\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(total_bid_volume - total_ask_volume)/(total_bid_volume + total_ask_volume)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"total_bid_volume / total_bid_volume\")\n",
    "    df['spread_intensity'] = df['spread'].diff()\n",
    "    df['market_urgency'] = df['spread'] * df['liquidity_imbalance']\n",
    "    df['order_book_imbalance'] = (df['total_ask_volume']-df['total_bid_volume'])/(df['total_ask_volume']+df['total_bid_volume']+1e-6)\n",
    "\n",
    "bid_volume_cols = [f'BidVolume{i}' for i in range(1, 11)]\n",
    "ask_volume_cols = [f'AskVolume{i}' for i in range(1, 11)]\n",
    "bid_price_cols = [f'BidPrice{i}' for i in range(1, 11)]\n",
    "ask_price_cols = [f'AskPrice{i}' for i in range(1, 11)]\n",
    "for df_features in [df_002521_signals, df_300132_signals]:\n",
    "    df_features['weighted_bid_price'] = (\n",
    "    df_features[bid_price_cols].mul(df_features[bid_volume_cols]).sum(axis=1) /\n",
    "    (df_features['total_bid_volume'] + 1e-6)\n",
    ")\n",
    "    df_features['weighted_ask_price'] = (\n",
    "    df_features[ask_price_cols].mul(df_features[ask_volume_cols]).sum(axis=1) /\n",
    "    (df_features['total_ask_volume'] + 1e-6)\n",
    ")\n",
    "    df_features['bid_price_gap'] = df_features['weighted_bid_price'] - df_features['BidPrice1']\n",
    "    df_features['ask_price_gap'] = df_features['weighted_ask_price'] - df_features['AskPrice1']\n",
    "    for i in range(1, 10):\n",
    "        df_features[f'bid_price_diff_{i}_{i+1}'] = df_features[f'BidPrice{i}'] - df_features[f'BidPrice{i+1}']\n",
    "        df_features[f'ask_price_diff_{i}_{i+1}'] = df_features[f'AskPrice{i+1}'] - df_features[f'AskPrice{i}']\n",
    "    df_features['avg_bid_price'] = df_features[bid_price_cols].mean(axis=1)\n",
    "    df_features['avg_ask_price'] = df_features[ask_price_cols].mean(axis=1)\n",
    "    df_features['median_bid_price'] = df_features[bid_price_cols].median(axis=1)\n",
    "    df_features['median_ask_price'] = df_features[ask_price_cols].median(axis=1)\n",
    "    df_features['std_bid_price'] = df_features[bid_price_cols].std(axis=1)\n",
    "    df_features['std_ask_price'] = df_features[ask_price_cols].std(axis=1)\n",
    "    df_features['best_volume_ratio'] = df_features['BidVolume1'] / (df_features['AskVolume1'] + 1e-6)\n",
    "    df_features['volume_ratio'] = df_features['total_bid_volume'] / (df_features['total_ask_volume'] + 1e-6)\n",
    "\n",
    "    top3_bid_cols = [f'BidVolume{i}' for i in range(1, 4)]\n",
    "    top3_ask_cols = [f'AskVolume{i}' for i in range(1, 4)]\n",
    "    df_features['top3_bid_volume'] = df_features[top3_bid_cols].sum(axis=1)\n",
    "    df_features['top3_ask_volume'] = df_features[top3_ask_cols].sum(axis=1)\n",
    "    df_features['top3_volume_ratio'] = df_features['top3_bid_volume'] / (df_features['top3_ask_volume'] + 1e-6)\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        df_features[f'spread_level_{i}'] = df_features[f'AskPrice{i}'] - df_features[f'BidPrice{i}']\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
